{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4b68b3-ed5c-4cd5-b640-1143e005b30b",
   "metadata": {},
   "source": ["DocMate is AI chat assistant that answers your medical questions\n",
    ",lets login to hungging face with access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d0df58-7239-4867-bc6c-8443977966e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [ "Logged in as Akhil1905\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "from huggingface_hub import HfFolder, whoami\n",
    "\n",
    "hf_token = 'YOUR_TOKEN'\n",
    "HfFolder.save_token(hf_token)\n",
    "\n",
    "user = whoami()\n",
    "print(f\"Logged in as {user['name']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea0bee-5bd9-4002-ac52-b90fd007256d",
   "metadata": {},
   "source": [
    "In the below cell we consider a model from hugging face which will be by default FP32 precision \n",
    "Our goal is to compress this model with OpenVino \n",
    "Precisions considered are FP16, INT8, INT4\n",
    "\n",
    "Gemma 2b-It: Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44477b3-f9ee-4999-8b78-795c178043f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "2024-11-15 19:00:54.242406: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-15 19:00:54.253733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-15 19:00:54.269906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-15 19:00:54.269947: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-15 19:00:54.280851: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 19:00:54.965660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60ac24f78604b298c5cf97e583200a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/cache_utils.py:458: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/opt/app-root/lib64/python3.9/site-packages/optimum/exporters/openvino/model_patcher.py:496: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/opt/app-root/lib64/python3.9/site-packages/nncf/torch/dynamic_graph/wrappers.py:86: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  op1 = operator(*args, **kwargs)\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/cache_utils.py:443: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d726c2a97c444c37974dddb49ff7d776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/cache_utils.py:458: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/opt/app-root/lib64/python3.9/site-packages/optimum/exporters/openvino/model_patcher.py:496: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/opt/app-root/lib64/python3.9/site-packages/nncf/torch/dynamic_graph/wrappers.py:86: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  op1 = operator(*args, **kwargs)\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/cache_utils.py:443: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n",
      "`quantization_config` was not provided. In the future, please provide `quantization_config`\n",
      "Calibration dataset was not provided, assuming weight only quantization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (127 / 127)            │ 100% (127 / 127)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b391e16b378d4487bcd476dfc1de0fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e561cffcf954361b3d7b5d22c71e26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/cache_utils.py:458: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/opt/app-root/lib64/python3.9/site-packages/optimum/exporters/openvino/model_patcher.py:496: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/opt/app-root/lib64/python3.9/site-packages/nncf/torch/dynamic_graph/wrappers.py:86: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  op1 = operator(*args, **kwargs)\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/cache_utils.py:443: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64c7c6012f24168971d8cc74f067afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 38% (30 / 127)              │ 21% (29 / 126)                         │\n",
      "├────────────────┼─────────────────────────────┼────────────────────────────────────────┤\n",
      "│              4 │ 62% (97 / 127)              │ 79% (97 / 126)                         │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c236ef5e7444abca20c46b31ef7e2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tf-keras==2.16.0\n",
    "!pip install nncf\n",
    "!pip install optimum.intel\n",
    "!pip install openvino\n",
    "model_id = 'google/gemma-2b-it'\n",
    "model_vendor, model_name = model_id.split('/')\n",
    "\n",
    "FP16_GEN, INT8_GEN, INT4_GEN = True, True, True\n",
    "\n",
    "if FP16_GEN:\n",
    "    from optimum.intel.openvino import OVModelForCausalLM\n",
    "    ov_model_int8=OVModelForCausalLM.from_pretrained(model_id=model_id, export=True, compile=False, load_in_8bit=False)\n",
    "    ov_model_int8.half()\n",
    "    ov_model_int8.save_pretrained(f'{model_name}/FP16')\n",
    "\n",
    "if INT8_GEN:\n",
    "    from optimum.intel.openvino import OVModelForCausalLM\n",
    "    from optimum.intel import OVQuantizer\n",
    "    ov_model_int8=OVModelForCausalLM.from_pretrained(model_id=model_id, export=True, compile=False, load_in_8bit=False)\n",
    "    quantizer = OVQuantizer.from_pretrained(ov_model_int8)\n",
    "    quantizer.quantize(save_directory=f'{model_name}/INT8', weights_only=True)\n",
    "\n",
    "if INT4_GEN:\n",
    "    import shutil\n",
    "    from optimum.intel.openvino import OVModelForCausalLM\n",
    "    import openvino as ov\n",
    "    import nncf\n",
    "    ov_model_int8=OVModelForCausalLM.from_pretrained(model_id=model_id, export=True, compile=False, load_in_8bit=False)\n",
    "    compressed_model = nncf.compress_weights(ov_model_int8.half()._original_model, mode=nncf.CompressWeightsMode.INT4_ASYM, group_size=128, ratio=0.8)\n",
    "    ov.save_model(compressed_model, f'{model_name}/INT4/openvino_model.xml')\n",
    "    shutil.copy(f'{model_name}/FP16/config.json', f'{model_name}/INT4/config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906e56a-5731-4e85-93bb-8f318f2c4875",
   "metadata": {},
   "source": [
    "Now a folder will be created after this step with compressed models in the form of OpenVino IR\n",
    "We can load these models by considering their respective directories.\n",
    "\n",
    "I am considering mainly FP16 and INT 8 models would like to use them interchangebaly depending on the complexity of task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534f08cc-094f-4ee2-81e3-4330a044fc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "import openvino as ov\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "\n",
    "core = ov.Core()\n",
    "model_vendor, model_name = model_id.split('/')\n",
    "\n",
    "model_precision = ['FP16', 'INT8', 'INT4', 'INT4_stateless'][1]\n",
    "\n",
    "model_dir=f'{model_name}/{model_precision}'\n",
    "tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "ov_config = {hints.performance_mode(): hints.PerformanceMode.LATENCY, streams.num(): \"1\", props.cache_dir(): \"\"}\n",
    "ov_model_int8 = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device='CPU',\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model_precision = ['FP16', 'INT8', 'INT4', 'INT4_stateless'][0]\n",
    "ov_model_fp16=OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device='CPU',\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print('Loaded 2 models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f4043c-228d-4e29-92c2-a863c8409265",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from 1st model INT 8:Explain about digestive system.\n",
      "\n",
      "Sure, here's a detailed explanation of the digestive system:\n",
      "\n",
      "**The digestive system** is a complex network of organs and tissues that work together to break down food, absorb nutrients, and eliminate waste products. It can be divided into two main parts: the **mechanical digestive system** and the **chemical digestive system**.\n",
      "\n",
      "**The mechanical digestive system** physically breaks down food into smaller molecules that can be absorbed by the body. This system includes the mouth, esophagus, stomach, and intestines.\n",
      "\n",
      "* **The mouth** is the first part of the digestive system. It breaks down food into smaller pieces by chewing and saliva.\n",
      "* **The esophagus** is a tube that carries food from the mouth to the stomach.\n",
      "* **The stomach** is a J-shaped organ that secretes gastric juices, which help to break down proteins.\n",
      "* **The intestines** are a series of long, coiled tubes that absorb nutrients from food that has been digested in the stomach.\n",
      "\n",
      "**The chemical digestive system** breaks down food into even smaller molecules using enzymes. This system includes the saliva, pancreas, liver, and intestines.\n",
      "\n",
      "* **The saliva** is a clear liquid that is produced by the salivary glands. It contains enzymes that help to break down carbohydrates.\n",
      "* **The pancreas** is an organ that is located behind the stomach. It secretes enzymes that help to break down proteins and carbohydrates.\n",
      "* **The liver** is the largest organ in the body. It\n",
      "Response from 2nd model FP 16:Explain about digestive system.\n",
      "\n",
      "Sure, here's a detailed explanation of the digestive system:\n",
      "\n",
      "**The digestive system** is a complex network of organs and tissues that work together to break down food, absorb nutrients, and eliminate waste products. It can be divided into two main parts: the **mechanical digestive system** and the **chemical digestive system**.\n",
      "\n",
      "**The mechanical digestive system** physically breaks down food into smaller molecules that can be absorbed by the body. This system includes the mouth, esophagus, stomach, and intestines.\n",
      "\n",
      "* **The mouth** is the first part of the digestive system. It breaks down food into smaller pieces by chewing and saliva.\n",
      "* **The esophagus** is a tube that carries food from the mouth to the stomach.\n",
      "* **The stomach** is a J-shaped organ that secretes gastric juices, which help to break down proteins.\n",
      "* **The intestines** are a series of long, coiled tubes that absorb nutrients from food that has been digested in the stomach.\n",
      "\n",
      "**The chemical digestive system** breaks down food into even smaller molecules using enzymes. This system includes the saliva, pancreas, liver, and intestines.\n",
      "\n",
      "* **The saliva** is a clear liquid that is produced by the salivary glands. It contains enzymes that help to break down carbohydrates.\n",
      "* **The pancreas** is an organ that is located behind the stomach. It secretes enzymes that help to break down proteins and carbohydrates.\n",
      "* **The liver** is the largest organ in the body. It\n"
     ]
    }
   ],
   "source": [
    "# Lets test the 2 models with same input\n",
    "prompt_text = 'Explain about digestive system.\\n\\n'\n",
    "input_tokens = tok.encode(prompt_text, return_tensors='pt')\n",
    "response = ov_model_int8.generate(input_tokens, max_new_tokens=300) \n",
    "response_text = tok.decode(response[0], skip_special_tokens=True)\n",
    "response_1=ov_model_fp16.generate(input_tokens, max_new_tokens=300)   \n",
    "response_text_1 = tok.decode(response_1[0], skip_special_tokens=True)\n",
    "print(f'Response from 1st model INT 8:{response_text}')\n",
    "print(f'Response from 2nd model FP 16:{response_text_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e77cd5-6894-4610-8b63-25ce74acf804",
   "metadata": {},
   "source": [
    "Here we can observe both the reponses are pretty similar because we asked a pretty simple quetion. The outputs will be visibly differ if we give any complex question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea7bc3-5cd5-4aeb-afc4-a9edc588f7f3",
   "metadata": {},
   "source": [
    "The below code is used for implementing retieval through PubMed Search\n",
    "PubMed is a free, publicly available search engine for biomedical and life sciences literature: \n",
    "What it contains\n",
    "PubMed contains over 37 million citations and abstracts from journals in medicine, nursing, dentistry, veterinary medicine, and other related fields. It also includes links to full text articles when available. \n",
    "Who developed it\n",
    "PubMed was developed and is maintained by the National Center for Biotechnology Information (NCBI) at the National Library of Medicine (NLM) at the National Institutes of Health (NIH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e42539f-8b07-40ed-8f9a-6eea34f71c17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.84-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from biopython) (1.26.4)\n",
      "Installing collected packages: biopython\n",
      "Successfully installed biopython-1.84\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Title: Cowpea mosaic virus intratumoral immunotherapy maintains stability and efficacy after long-term storage.\n",
      "Abstract: Cowpea mosaic virus (CPMV) has demonstrated superior immune stimulation and efficacy as an intratumoral immunotherapy, providing a strong argument for its clinical translation. One important consideration for any new drug candidate is the long-term stability of the drug and its formulation. Therefore, our lab has evaluated the physical stability and biological activity, that is, anti-tumor potency, of formulations of CPMV in buffer (with and without a sucrose preservative) in multiple temperature conditions ranging from ultralow freezers to a heated incubator over a period of 9 months. We found that non-refrigerated temperatures 37°C and room temperature quickly led to CPMV destabilization, as evidenced by significant protein and RNA degradation after just 1 week. Refrigerated storage at 4°C extended physical stability, though signs of particle breakage and RNA escape appeared after 6 and 9 months. CPMV stored in frozen conditions, including -20°C, -80°C, and liquid N<sub>2</sub>, remained intact and matched the characteristics of fresh CPMV throughout the duration of the study. The biological activity was evaluated using a murine dermal melanoma model, and efficacy followed the observed trends in physical stability: CPMV stored in refrigerated and warmer conditions exhibited decreased anti-tumor efficacy compared to freshly prepared formulations. Meanwhile, frozen-stored CPMV performed similarly to freshly purified CPMV, resulting in reduced tumor growth and extended survival. Data, therefore, indicates that CPMV stored long-term in cold or frozen conditions remains stable and efficacious, providing additional support to advance this powerful plant virus to translation.\n",
      "\n",
      "---\n",
      "\n",
      "Title: Chemotherapy-induced increase in CD47 expression in epithelial ovarian cancer.\n",
      "Abstract: Epithelial ovarian cancer (EOC) remains the most lethal gynecological malignancy with limited treatment options. CD47 is a critical immune checkpoint for tumor immune evasion and has been targeted in various clinical trials. This study aimed to assess the impact of chemotherapy on CD47 expression in EOC in order to determine the potential and optimal timing of CD47-targeted therapy in ovarian cancer. We analyzed the expression of CD47 in ovarian cancer and the effect of chemotherapy on the expression of CD47 in ovarian cancer tissues. Furthermore, we investigated the effect of chemotherapy on the expression of CD47 in ovarian cancer cells.\n",
      "\n",
      "---\n",
      "\n",
      "Title: Mitochondrial regulation in the tumor microenvironment: targeting mitochondria for immunotherapy.\n",
      "Abstract: Mitochondrial regulation plays a crucial role in cancer immunity in the tumor microenvironment (TME). Infiltrating immune cells, including T cells, natural killer (NK) cells, and macrophages, undergo mitochondrial metabolic reprogramming to survive the harsh conditions of the TME and enhance their antitumor activity. On the other hand, immunosuppressive cells like myeloid-derived suppressor cells (MDSCs), regulatory T cells (Tregs), mast cells, and tumor-associated macrophages (TAMs) rely on mitochondrial regulation to maintain their function as well. Additionally, mitochondrial regulation of cancer cells facilitates immune evasion and even hijacks mitochondria from immune cells to enhance their function. Recent studies suggest that targeting mitochondria can synergistically reduce cancer progression, especially when combined with traditional cancer therapies and immune checkpoint inhibitors. Many mitochondrial-targeting drugs are currently in clinical trials and have the potential to enhance the efficacy of immunotherapy. This mini review highlights the critical role of mitochondrial regulation in cancer immunity and provides lists of mitochondrial targeting drugs that have potential to enhance the efficacy of cancer immunotherapy.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def pubmed_search(query):\n",
    "    \"\"\"\n",
    "    Search PubMed for articles matching the query and return the articles' titles and abstracts.\n",
    "    \n",
    "    Params:\n",
    "    query (str): The search query to look for articles in PubMed.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tuples containing article titles and abstracts.\n",
    "    \"\"\"\n",
    "    # Set your email (required by NCBI API)\n",
    "    Entrez.email = \"YOUR_EMAIL\"\n",
    "\n",
    "    # Perform the PubMed search\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=3)  # Limit to 3 articles for simplicity\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    # Get the list of PubMed article IDs (PMID)\n",
    "    pmids = record[\"IdList\"]\n",
    "    articles = []\n",
    "\n",
    "    # Fetch details for each article using the IDs\n",
    "    if pmids:\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"xml\", retmode=\"text\")\n",
    "        records = Entrez.read(handle)\n",
    "        handle.close()\n",
    "\n",
    "        # Process each record\n",
    "        for article in records[\"PubmedArticle\"]:\n",
    "            title = article[\"MedlineCitation\"][\"Article\"][\"ArticleTitle\"]\n",
    "            try:\n",
    "                abstract = article[\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"][0]\n",
    "            except KeyError:\n",
    "                abstract = \"No abstract available.\"\n",
    "\n",
    "            # Truncate abstract to a maximum length of 400 words\n",
    "            truncated_abstract = truncate(abstract)\n",
    "\n",
    "            # Append the article title and truncated abstract to the result list\n",
    "            articles.append((title, truncated_abstract))\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "def truncate(text):\n",
    "    \"\"\"\n",
    "    Truncate text to 400 words.\n",
    "\n",
    "    Params:\n",
    "    text (str): The text to be truncated.\n",
    "\n",
    "    Returns:\n",
    "    str: The truncated text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return \" \".join(words[:400])\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "query = \"cancer immunotherapy\"\n",
    "articles = pubmed_search(query)\n",
    "\n",
    "for title, abstract in articles:\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Abstract: {abstract}\")\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db615c4-ff6b-466a-a754-55a1d6114fe7",
   "metadata": {},
   "source": [
    "The above are few articles printed for a search through PubMed:\n",
    "The format it provides contains a title and an abstract about that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c36bf4c-1f12-4659-b0e5-cd6d83ac7bb7",
   "metadata": {},
   "source": [
    "Below I have Implemented the Core logic for the chat application :\n",
    "\n",
    "The main idea is to intent based classification of user input\n",
    "\n",
    "For General Purpose which is basically handling genral medical questions I have considered INT8 model Which can handle efficiently\n",
    "\n",
    "For more sophisticated responses like summarizing or answering from PubMed Search or Disesasem prediction I have used FP16 \n",
    "\n",
    "The usage of this Hybrid approach makes the application a bit more efficient in handling user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2459287a-e3c0-4596-bd2e-319cbd0f59f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install gradio\n",
    "# !pip install langchain_community\n",
    "import gradio as gr\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "\n",
    "\n",
    "\n",
    "def process_response(text):\n",
    "    \"\"\"Helper function to process Gemini responses\"\"\"\n",
    "    input_tokens = tok.encode(text, return_tensors='pt')\n",
    "    response = ov_model_int8.generate(input_tokens, max_new_tokens=300,temperature=0.3)   # Other options: temperature=1.0, do_sample=True, top_k=5, top_p=0.85, repetition_penalty=1.2)\n",
    "    response_text = tok.decode(response[0], skip_special_tokens=True)\n",
    "    return response_text.replace(text,'')\n",
    "\n",
    "def get_disease(input_text):\n",
    "    # Extract symptoms\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a medical chat assistant. The user has reported symptoms, and based on this information, you should:\n",
    "1. Predict the most likely disease.\n",
    "2. Highlight any additional symptoms that might be related to the disease.\n",
    "3. Provide the top 5 remedies with a brief explanation for each remedy.\n",
    "Please ensure that the response is clear, concise, and appropriate for a non-technical audience.\n",
    "User reported symptoms: {input_text}\n",
    "Note: You should not generate user specific responses or prompt in th final response\n",
    "\"\"\"\n",
    "   \n",
    "    input_tokens = tok.encode(prompt, return_tensors='pt')\n",
    "    response = ov_model_fp16.generate(input_tokens, max_new_tokens=300)   # Other options: temperature=1.0, do_sample=True, top_k=5, top_p=0.85, repetition_penalty=1.2)\n",
    "    response_text = tok.decode(response[0], skip_special_tokens=True)\n",
    "   \n",
    "    response_text.replace('User Input:','')\n",
    "    response_text.replace(input_text,'')\n",
    "    \n",
    "    return response_text.replace(prompt,'')\n",
    "def get_scimed(input_text):\n",
    "    articles = pubmed_search(input_text)\n",
    "    articles_text = \"\\n\".join(articles)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a medical chat assistant.\n",
    "    Based on the articles below, provide information for the user’s query in a friendly and professional manner.\n",
    "    user' query:{input_text}\n",
    "    Articles:\n",
    "    {articles_text}\n",
    "    \n",
    "    \"\"\"\n",
    "    input_tokens = tok.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    response = ov_model_fp16.generate(input_tokens, max_new_tokens=300,do_sample=True)   # Other options: temperature=1.0, do_sample=True, top_k=5, top_p=0.85, repetition_penalty=1.2)\n",
    "    response_text = tok.decode(response[0], skip_special_tokens=True)\n",
    "    print(response_text)\n",
    "    response_text.replace('User Input:','')\n",
    "    response_text.replace(input_text,'')\n",
    "    ls=[]\n",
    "    \n",
    "    for title in articles:\n",
    "        ls.append(title)\n",
    "        \n",
    "    return response_text.replace(prompt,''),ls\n",
    "\n",
    "\n",
    "def chat_response(message, history):\n",
    "    # Classify the user input based on intent\n",
    "    intent_prompt = f'''Please classify the following input into one of the categories below. Analyze the context and nuances of the input carefully to ensure accurate classification.\n",
    "\n",
    "**Categories:**\n",
    "1. **New Symptom**: Use this category if the user describes symptoms they are currently experiencing, without asking a question about a symptom, disease, or medical condition in general. This is typically when the user is directly seeking help for their own health concerns (e.g., \"I have a sore throat and feel dizzy\" or \"I'm experiencing joint pain\").\n",
    "\n",
    "2. **Clarification**: Use this category if the user asks for specific clarification on a medical topic, symptom, or term they don’t fully understand. This usually involves requests for explanation, interpretation, or more information about something specific (e.g., \"What does hypertension mean?\" or \"Can you clarify the side effects of this drug?\").\n",
    "\n",
    "3. **General Info Request**: Use this category if the user asks general medical questions, seeks information on diseases, or makes inquiries that do not involve their own symptoms. This includes questions about conditions, lifestyle advice, and preventive health measures (e.g., \"What causes diabetes?\" or \"How can I improve my immune system?\").\n",
    "\n",
    "4. **Scientific Doubt**: Use this category for inquiries related to recent or advanced scientific research, experimental treatments, new technologies, or cutting-edge findings in the medical field. This typically involves complex or specialized questions (e.g., \"What are the latest developments in cancer immunotherapy?\" or \"How does CRISPR technology work in treating genetic diseases?\").\n",
    "\n",
    "5. **Other**: Use this category for all other types of inquiries that do not fit the above categories. This may include conversational or off-topic messages, general greetings, or questions outside the medical domain (e.g., \"Hello, how are you?\" or \"Can you help me with a non-medical question?\").\n",
    "\n",
    "**Instructions**:\n",
    "- Focus on the context and intent behind the input to classify it correctly.\n",
    "- Consider subtle differences; for example, distinguish between asking for information about a symptom (General Info Request) and describing one’s own symptoms (New Symptom).\n",
    "- Only provide the class name.\n",
    "\n",
    "Input: {message}'''\n",
    "\n",
    "\n",
    "    # Get the intent by processing the classification prompt\n",
    "    intent = process_response(intent_prompt)\n",
    "    intent = intent.strip()  # Clean up the output\n",
    "    print(intent)\n",
    "    \n",
    "    # Now generate a response based on the classified intent\n",
    "    if 'New Symptom' in intent:\n",
    "        # Generate a response and update the history\n",
    "        response = get_disease(message)  \n",
    "        history.append((\"user\", message))\n",
    "        history.append((\"bot\", response))\n",
    "        \n",
    "    elif 'Clarification' in intent:\n",
    "        # Provide a clarification response while keeping the tone friendly\n",
    "        prompt = f'You are a doctor. Clarify this request: {message}. Sound friendly and professional.'\n",
    "        response = process_response(prompt)\n",
    "        history.append((\"user\", message))\n",
    "        history.append((\"bot\", response))\n",
    "        \n",
    "    elif 'General Info Request' in intent:\n",
    "        # Provide detailed medical information in an easy-to-understand format\n",
    "        prompt = f'You are a doctor. Provide detailed yet easy-to-understand medical info for: {message}'\n",
    "        response = process_response(prompt)\n",
    "        history.append((\"user\", message))\n",
    "        history.append((\"bot\", response))\n",
    "        \n",
    "    elif 'Scientific Doubt' in intent:\n",
    "        # Handle scientific-related medical inquiries\n",
    "        prompt = f'You are a doctor. Provide scientific details about: {message}.'\n",
    "        response,references = get_scimed(prompt)\n",
    "        response, references = get_scimed(prompt)\n",
    "        if references:\n",
    "            response += \"\\n\\nReferences:\\n\"\n",
    "            for reference in references:\n",
    "                response += f\"- {reference}\\n\"\n",
    "        history.append((\"user\", message))\n",
    "        history.append((\"bot\", response))\n",
    "\n",
    "    else:  # Other\n",
    "        # Placeholder for better output \n",
    "        response = ' I am a medical Assistant designed specifically for medical use cases. I may not be good one to answer the specific question. Kindly provide any medical related queries'\n",
    "        history.append((\"user\", message))\n",
    "        history.append((\"bot\", response))\n",
    "\n",
    "    # Update history with the latest response\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27ac09-8d77-4b8c-8e62-c3c980d6cd10",
   "metadata": {},
   "source": [
    "Each of these functions with specific prompts helps in efficient and a bit accurate responsed to the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7260ccc4-cf11-4f51-88d3-29cbef0079d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://9d8816331f52b5cea8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9d8816331f52b5cea8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Answer:** 4. Scientific Doubt\n",
      "\n",
      "The question is about the latest developments in cancer immunotherapy, which is a scientific research area.\n",
      "\n",
      "    You are a medical chat assistant.\n",
      "    Based on the articles below, provide information for the user’s query in a friendly and professional manner.\n",
      "    user' query:You are a doctor. Provide scientific details about: Latest cancer treatments?.\n",
      "    Articles:\n",
      "    \n",
      "    \n",
      "    1. **Recent Clinical Trials on Cancer Treatment** published in \"The Lancet\" reports promising results of a new targeted therapy for leukemia. The therapy, called Pembrolizumab (MK-452), significantly improved the survival rate of patients with aggressive leukemia and was found to be safe and well-tolerated. \n",
      "\n",
      "    2. **New Drug Approvals for Cancer Treatment** published in \"The New England Journal of Medicine\" announced the approval of a novel immunotherapy drug called Nivolumab (Opdivo) for treating multiple myeloma. Nivolumab is a chimeric antigen receptor (CAR) drug targeting the CD33 protein on leukemia cells. \n",
      "\n",
      "  Please provide a summary of the articles, highlighting the scientific details and conclusions reached.\n",
      "\n",
      "Sure, here's a summary:\n",
      "\n",
      "**Recent Clinical Trials on Cancer Treatment**\n",
      "\n",
      "* Pembrolizumab (MK-452) is a targeted therapy for leukemia that significantly improved the survival rate of patients with aggressive leukemia.\n",
      "* Pembrolizumab was found to be safe and well-tolerated with minimal side effects.\n",
      "\n",
      "\n",
      "**New Drug Approvals for Cancer Treatment**\n",
      "\n",
      "* Nivolumab (Opdivo) is an immunotherapy drug that was approved for the treatment of multiple myeloma.\n",
      "* Nivolumab is a CAR drug targeting the CD33 protein on leukemia cells.\n",
      "* Nivolumab showed promising results in clinical trials and was well-tolerated.\n",
      "\n",
      "    You are a medical chat assistant.\n",
      "    Based on the articles below, provide information for the user’s query in a friendly and professional manner.\n",
      "    user' query:You are a doctor. Provide scientific details about: Latest cancer treatments?.\n",
      "    Articles:\n",
      "    \n",
      "    \n",
      "    _**Scientific Breakthroughs in Cancer Treatment: New Drug and Immunotherapy Combinations Are Showing Promise in Treatment of Breast Cancer**_ (Nature, 2023)\n",
      "\n",
      "    _**The Latest Cancer Treatment: Cutting-Edge Advances for Breast Cancer**_ (GoodNewsHealth, 2023)\n",
      "\n",
      "**Scientific details:**\n",
      "\n",
      "The articles provide the following insights into the latest cancer treatments:\n",
      "\n",
      "- Development of new drug and immunotherapy combinations for treating breast cancer, which holds the potential to improve patient outcomes.\n",
      "- Advancements and potential breakthroughs in cancer immunotherapy, including the use of nanotechnology and personalized medicine.\n",
      "- Use of artificial intelligence and data analytics for personalized treatment plans.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The latest scientific breakthroughs suggest that cancer treatments are rapidly evolving, with promising new combinations and advancements that offer hope for patients battling breast cancer.\n",
      ".\n",
      "\n",
      "**Classification:** New Symptom\n"
     ]
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# DocMate a Chat Assistant\n",
    "    👋 Welcome to the DocMate Chat Assistant.  \n",
    "    Your personal medical chat assistant, connected to the internet world.🌎\n",
    "    \"\"\")\n",
    "    \n",
    "    chatbot = gr.ChatInterface(\n",
    "        fn=chat_response,\n",
    "        examples=[\n",
    "            \"I have fever, headache, and body pain\",\n",
    "            \"Can you explain what causes high blood pressure?\",\n",
    "            \"Latest cancer treatments?\"\n",
    "        ],\n",
    "        title=\"Powered by OpenVino with Gemma 2B\",\n",
    "        retry_btn=None,\n",
    "        undo_btn=None,\n",
    "        clear_btn=\"Clear\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553632e-1696-4334-b7f4-0aa4f1e081cd",
   "metadata": {},
   "source": [
    "The chat application can be interacted via gradio interface and the public URL generated.\n",
    "\n",
    "The application considers the users symptoms and predicts most likely disease from it. It also provides additional symptoms that user can counter verify if they have these additional symptoms or not. Finally our application provides remedies for temporary relief for the user's symptoms. The other thing it is capable of is it can handle queries that requires latest medical treatments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
